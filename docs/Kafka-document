1) Design a data pipeline that uses Kafka for real-time data processing and storage?
    a. Define the requirements:
    b. Choose the Kafka cluster configuration
    c. Configure the data sources
    d. Define the data processing logic: I would define the data processing logic using Kafka Streams or other stream
        processing frameworks, such as Spark Streaming or Flink. I would define the data processing pipeline, including data enrichment, filtering, aggregation, and transformation.
    e. Choose the storage layer:
    f. Monitor and optimize the data pipeline:


    Data Sources -> Kafka Producers -> Kafka Cluster -> Kafka Consumers -> Processing & Storage Systems

   =   This could be important use case, I like this : By integrating your machine learning model into a Kafka-based data pipeline
   =   Set up the Kafka cluster
         - You'll need at least three servers to set up a Kafka cluster
           (one for the ZooKeeper ensemble and two for Kafka brokers).

         - Install ZooKeeper: Kafka relies on ZooKeeper for coordination and management of the Kafka brokers.
         - Install Kafka: Once you've installed ZooKeeper, you can install Kafka on your Kafka broker servers.
         - Configure Kafka: After you've installed Kafka, you'll need to configure it by editing the server.properties file on each Kafka broker server
         - Start the Kafka brokers: Once you've configured Kafka.
         - Test the Kafka cluster: To test that your Kafka cluster is working correctly, you can create a topic, produce messages to the topic, and consume messages from the topic.


     2) Does the replicated topic,  maintains offsets as well.

         In a replicated topic in Apache Kafka, the offsets are also maintained along with the message data. When a message is produced to a Kafka topic, it is assigned a unique offset that represents its position within the partition. This offset is used to track the progress of consumers as they read messages from the topic.
         In a replicated topic, the offsets are replicated across all replicas of the partition. This ensures that each replica maintains the same sequence of messages and that they all have the same offset values. This is important for ensuring data consistency and fault tolerance in the Kafka cluster.`
         Additionally, Kafka also provides a mechanism for consumers to commit their current offset position back to the Kafka broker, which allows them to resume reading from where they left off in case of a failure or restart. This offset commit is also replicated across all replicas of the partition to ensure that all consumers have the same view of the current offset position.


    3) How can the client find out which topics exist, what partitions they have, and which brokers currently host those partitions so that it can direct its requests to the right hosts